{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"BSG Stroke Classifier","text":"<p>Welcome to the documentation for the BSG Stroke Classifier, a machine learning project designed to predict the likelihood of stroke events using patient medical records. This project focuses on the analysis, preprocessing, and modeling of healthcare data to build a reliable binary classifier.</p>"},{"location":"#project-overview","title":"Project Overview","text":"<p>The goal of this project is to explore a healthcare dataset and build an end-to-end classification pipeline for stroke prediction. It includes:</p> <ul> <li>Data cleaning and imputation of missing values.</li> <li>Exploratory Data Analysis (EDA) to understand feature relationships.</li> <li>Preprocessing, including encoding and scaling.</li> <li>Machine learning modeling using classification algorithms.</li> <li>Evaluation metrics to assess performance.</li> <li>A clean and reproducible structure using Python, Poetry, and MkDocs.</li> </ul>"},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>bsg-testdatascience-1/\n\u251c\u2500\u2500 data/ # Raw and processed data files\n\u251c\u2500\u2500 docs/ # Documentation (MkDocs)\n\u251c\u2500\u2500 notebooks/ # EDA and experimentation notebooks\n\u251c\u2500\u2500 stroke_predictor/ # Source code and utility functions\n\u2502 \u251c\u2500\u2500 configs/ # YAML configurations\n\u2502 \u251c\u2500\u2500 optuna_optimization/ # Optuna-based model optimization\n\u2502 \u251c\u2500\u2500 utils/ # Preprocessing, plotting, and I/O helpers\n\u2502 \u2514\u2500\u2500 tests/ # Unit tests\n\u251c\u2500\u2500 templates/ # HTML templates for the web interface\n\u251c\u2500\u2500 pyproject.toml # Poetry config\n\u251c\u2500\u2500 mkdocs.yml # MkDocs config\n\u2514\u2500\u2500 README.md # Project overview\n</code></pre>"},{"location":"#documentation-contents","title":"Documentation Contents","text":"<ul> <li>Exploratory Data Analysis \u2013 Summary statistics, visualizations, and patterns in the dataset.</li> <li>Preprocessing Utilities \u2013 Code for missing value imputation, encoding, and normalization.</li> <li>Modeling Pipeline \u2013 Train/test splits, model training, and evaluation.</li> <li>Testing \u2013 Unit tests for critical components.</li> <li>API Reference \u2013 Autogenerated from code docstrings using MkDocs + <code>mkdocstrings</code>.</li> <li>Hyperparameter Optimization \u2013 Tuning with Optuna.</li> <li>Web Interface \u2013 Simple UI for predictions via Flask.</li> </ul>"},{"location":"#how-to-use","title":"How to Use","text":"<ol> <li> <p>Clone the repository:    <code>bash    git clone https://github.com/borja-sg/BSG-TestDataScience-1    cd BSG-TestDataScience-1</code></p> </li> <li> <p>Install dependencies using Poetry:    <code>bash    poetry install</code></p> </li> <li> <p>Run notebooks or scripts from the root folder.</p> </li> </ol>"},{"location":"#running-steps","title":"Running Steps","text":"<ol> <li> <p>Exploratory Data Analysis (EDA)    Run the notebook to perform data cleaning, seasonality analysis, and feature engineering: <code>bash    poetry run jupyter notebook</code>    Open and execute <code>notebooks/eda.ipynb</code>.</p> </li> <li> <p>Optuna Hyperparameter Optimization    Run automated model tuning with Optuna: <code>bash    poetry run python series_predictor/optuna_optimization/runner.py ./series_predictor/configs/optuna.yml</code>    Training progress and results are logged with MLflow.</p> </li> <li> <p>Model Evaluation    Evaluate the trained model using: <code>bash    poetry run jupyter notebook</code>    Then open and run <code>notebooks/evaluation.ipynb</code>.</p> </li> </ol>"},{"location":"api/","title":"\ud83d\udd27 API Reference","text":"<p>This section contains the auto-generated documentation for the main utility modules in the <code>stroke_predictor</code> package.</p>"},{"location":"api/#stroke_predictorutilsdata_io","title":"\ud83d\udcc2 <code>stroke_predictor.utils.data_io</code>","text":""},{"location":"api/#stroke_predictor.utils.data_io.load_column_names","title":"<code>load_column_names(path, key)</code>","text":"<p>Load column names from an HDF5 file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to the HDF5 file.</p> required <code>key</code> <code>str</code> <p>Key under which the DataFrame is stored.</p> required <p>Returns:</p> Type Description <code>list</code> <p>List of column names in the DataFrame.</p> Source code in <code>stroke_predictor/utils/data_io.py</code> <pre><code>def load_column_names(path: Path, key: str) -&gt; list:\n    \"\"\"\n    Load column names from an HDF5 file.\n\n    Parameters\n    ----------\n    path : Path\n        Path to the HDF5 file.\n    key : str\n        Key under which the DataFrame is stored.\n\n    Returns\n    -------\n    list\n        List of column names in the DataFrame.\n    \"\"\"\n\n    # Check if the path exists and is a file\n    if not path.exists():\n        raise FileNotFoundError(f\"The file '{path}' does not exist.\")\n    if not path.is_file():\n        raise ValueError(f\"'{path}' is not a file (may be a directory).\")\n\n    # Load the DataFrame and return its columns\n    df = pd.read_hdf(path, key=key)\n\n    # Drop stroke\n    if \"stroke\" in df.columns:\n        df = df.drop(\"stroke\", axis=1)  # Exclude stroke column if it exists\n    return df.columns.tolist()\n</code></pre>"},{"location":"api/#stroke_predictor.utils.data_io.load_config","title":"<code>load_config(model_path)</code>","text":"<p>Load model configuration and related resources.</p> Source code in <code>stroke_predictor/utils/data_io.py</code> <pre><code>def load_config(model_path: str) -&gt; dict:\n    \"\"\"Load model configuration and related resources.\"\"\"\n    config_path = Path(model_path) / \"model_config.yml\"\n    if not config_path.exists():\n        raise FileNotFoundError(f\"Model configuration file not found at {config_path}\")\n\n    with open(config_path, \"r\") as f:\n        config = yaml.safe_load(f)\n\n    variable_encoder_path = config[\"storage\"].get(\"variable_encoder_path\", None)\n    if variable_encoder_path and os.path.exists(variable_encoder_path):\n        variable_encoder = joblib.load(variable_encoder_path)\n    else:\n        raise FileNotFoundError(\n            f\"Variable encoder not found at {variable_encoder_path}\"\n        )\n\n    path_data = Path(config[\"storage\"][\"data_path\"])\n    if not path_data.exists():\n        raise FileNotFoundError(f\"Data file not found at {path_data}\")\n\n    key = config[\"storage\"][\"hdf5_key_train\"]\n    column_names = load_column_names(path=path_data, key=key)\n\n    return {\n        \"model_path\": model_path,\n        \"variable_encoder\": variable_encoder,\n        \"column_names\": column_names,\n    }\n</code></pre>"},{"location":"api/#stroke_predictor.utils.data_io.load_csv_dataset","title":"<code>load_csv_dataset(path)</code>","text":"<p>Load a CSV file as a pandas DataFrame. Raises an error if the file does not exist.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the CSV file.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Loaded DataFrame.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified file does not exist.</p> <code>ValueError</code> <p>If the path is not a file (e.g., a directory).</p> Source code in <code>stroke_predictor/utils/data_io.py</code> <pre><code>def load_csv_dataset(path: Path) -&gt; pd.DataFrame:\n    \"\"\"\n    Load a CSV file as a pandas DataFrame. Raises an error if the file does not exist.\n\n    Parameters\n    ----------\n    path : str\n        Path to the CSV file.\n\n    Returns\n    -------\n    pd.DataFrame\n        Loaded DataFrame.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the specified file does not exist.\n    ValueError\n        If the path is not a file (e.g., a directory).\n    \"\"\"\n\n    # Check if the path exists and is a file\n    if not path.exists():\n        raise FileNotFoundError(f\"The file '{path}' does not exist.\")\n    if not path.is_file():\n        raise ValueError(f\"'{path}' is not a file (may be a directory).\")\n\n    # Load and return the DataFrame\n    return pd.read_csv(path)\n</code></pre>"},{"location":"api/#stroke_predictor.utils.data_io.load_dataset","title":"<code>load_dataset(path, key, target)</code>","text":"<p>Load a DataFrame from an HDF5 file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to the HDF5 file.</p> required <code>key</code> <code>str</code> <p>Key under which the DataFrame is stored.</p> required <code>target</code> <code>str</code> <p>Name of the target variable column in the DataFrame.</p> required <p>Returns:</p> Name Type Description <code>x</code> <code>ndarray</code> <p>Features from the DataFrame.</p> <code>y</code> <code>ndarray</code> <p>Target variable from the DataFrame.</p> Source code in <code>stroke_predictor/utils/data_io.py</code> <pre><code>def load_dataset(path: Path, key: str, target: str) -&gt; Tuple:\n    \"\"\"\n    Load a DataFrame from an HDF5 file.\n\n    Parameters\n    ----------\n    path : Path\n        Path to the HDF5 file.\n    key : str\n        Key under which the DataFrame is stored.\n    target : str\n        Name of the target variable column in the DataFrame.\n\n    Returns\n    -------\n    x : np.ndarray\n        Features from the DataFrame.\n    y : np.ndarray\n        Target variable from the DataFrame.\"\"\"\n\n    # Check if the path exists and is a file\n    if not path.exists():\n        raise FileNotFoundError(f\"The file '{path}' does not exist.\")\n    if not path.is_file():\n        raise ValueError(f\"'{path}' is not a file (may be a directory).\")\n\n    # Load and return the DataFrame from HDF5\n    df = pd.read_hdf(path, key=key)\n\n    # Check if the target column exists in the DataFrame\n    if target not in df.columns:\n        raise ValueError(f\"Target column '{target}' not found in the DataFrame.\")\n\n    # Separate features and target variable\n    x = df.drop(columns=[target]).values\n    y = df[target].values\n    return x, y\n</code></pre>"},{"location":"api/#stroke_predictor.utils.data_io.load_model_and_config","title":"<code>load_model_and_config(model_path)</code>","text":"<p>Load the ML model, configuration file, variable encoder, and column names.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the MLflow model directory.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the model, model config, column names, and variable encoder.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If any of the required files (config, encoder, data) are missing.</p> Source code in <code>stroke_predictor/utils/data_io.py</code> <pre><code>def load_model_and_config(model_path: str) -&gt; Tuple[Any, dict, list[str], Any]:\n    \"\"\"\n    Load the ML model, configuration file, variable encoder, and column names.\n\n    Parameters\n    ----------\n    model_path : str\n        Path to the MLflow model directory.\n\n    Returns\n    -------\n    tuple\n        A tuple containing the model, model config, column names, and variable encoder.\n\n    Raises\n    ------\n    FileNotFoundError\n        If any of the required files (config, encoder, data) are missing.\n    \"\"\"\n    config_path = Path(model_path) / \"model_config.yml\"\n    if not config_path.exists():\n        raise FileNotFoundError(f\"Model configuration file not found at {config_path}\")\n\n    with open(config_path, \"r\") as f:\n        model_config = yaml.safe_load(f)\n\n    variable_encoder_path = model_config[\"storage\"].get(\"variable_encoder_path\")\n    if not variable_encoder_path or not os.path.exists(variable_encoder_path):\n        raise FileNotFoundError(\n            f\"Variable encoder not found at {variable_encoder_path}\"\n        )\n\n    path_data = Path(model_config[\"storage\"][\"data_path\"])\n    if not path_data.exists():\n        raise FileNotFoundError(f\"Data file not found at {path_data}\")\n\n    key = model_config[\"storage\"][\"hdf5_key_train\"]\n    column_names = load_column_names(path=path_data, key=key)\n\n    # Now it's safe to load the model\n    model = mlflow.sklearn.load_model(model_path)\n    variable_encoder = joblib.load(variable_encoder_path)\n\n    return model, model_config, column_names, variable_encoder\n</code></pre>"},{"location":"api/#stroke_predictor.utils.data_io.save_to_hdf","title":"<code>save_to_hdf(df, path, key, mode)</code>","text":"<p>Save a DataFrame to an HDF5 file.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to save.</p> required <code>path</code> <code>Path</code> <p>Destination HDF5 file path.</p> required <code>key</code> <code>str</code> <p>Key under which the DataFrame is stored.</p> required <code>mode</code> <code>str</code> <p>Mode to open the file ('w' for write, 'a' for append), by default 'a'.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the mode is not 'w' or 'a'.</p> <code>TypeError</code> <p>If the key is not a string.</p> Source code in <code>stroke_predictor/utils/data_io.py</code> <pre><code>def save_to_hdf(\n    df: pd.DataFrame, path: Path, key: str, mode: Literal[\"a\", \"w\", \"r+\"]\n) -&gt; None:\n    \"\"\"\n    Save a DataFrame to an HDF5 file.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        DataFrame to save.\n    path : Path\n        Destination HDF5 file path.\n    key : str\n        Key under which the DataFrame is stored.\n    mode : str, optional\n        Mode to open the file ('w' for write, 'a' for append), by default 'a'.\n\n    Raises\n    ------\n    ValueError\n        If the mode is not 'w' or 'a'.\n    TypeError\n        If the key is not a string.\n    \"\"\"\n\n    # Create parent directories if they don't exist\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Validate mode\n    if mode not in [\"w\", \"a\", \"r+\"]:\n        raise ValueError(\"Mode must be either 'a' or 'w', 'r+'\")\n\n    # Validate key\n    if not isinstance(key, str):\n        raise TypeError(\"Key must be a string\")\n\n    # Check if the path folder exists\n    if not path.parent.exists():\n        raise FileNotFoundError(f\"The directory '{path.parent}' does not exist.\")\n\n    # Save the DataFrame to HDF5\n    df.to_hdf(path, key=key, mode=mode, format=\"table\")\n</code></pre>"},{"location":"api/#stroke_predictorutilsplotting","title":"\ud83d\udcc2 <code>stroke_predictor.utils.plotting</code>","text":""},{"location":"api/#stroke_predictor.utils.plotting.plot_categorical_counts","title":"<code>plot_categorical_counts(df, column, hue=None, normalize=False, title=None, y_log=False, output_path=None)</code>","text":"<p>Plot the counts of a categorical column using seaborn.countplot.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>column</code> <code>str</code> <p>Column name to count.</p> required <code>hue</code> <code>str</code> <p>Column to use for bar color separation.</p> <code>None</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize counts per group (hue-level).</p> <code>False</code> <code>title</code> <code>str</code> <p>Title of the plot.</p> <code>None</code> <code>output_path</code> <code>str</code> <p>Path to save the plot image. If None, the plot is shown interactively.</p> <code>None</code> Source code in <code>stroke_predictor/utils/plotting.py</code> <pre><code>def plot_categorical_counts(\n    df: pd.DataFrame,\n    column: str,\n    hue: Optional[str] = None,\n    normalize: bool = False,\n    title: Optional[str] = None,\n    y_log: bool = False,\n    output_path: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Plot the counts of a categorical column using seaborn.countplot.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Input DataFrame.\n    column : str\n        Column name to count.\n    hue : str, optional\n        Column to use for bar color separation.\n    normalize : bool, optional\n        Whether to normalize counts per group (hue-level).\n    title : str, optional\n        Title of the plot.\n    output_path : str, optional\n        Path to save the plot image. If None, the plot is shown interactively.\n    \"\"\"\n    plt.figure(figsize=(8, 4))\n    data = df.copy()\n    if normalize and hue:\n        norm_df = (\n            data.groupby([column, hue], observed=False)\n            .size()\n            .div(data.groupby(column, observed=False).size(), level=column)\n            .reset_index(name=\"proportion\")\n        )\n        sns.barplot(data=norm_df, x=column, y=\"proportion\", hue=hue)\n        plt.ylabel(\"Proportion\")\n    else:\n        sns.countplot(data=data, x=column, hue=hue)\n        plt.ylabel(\"Counts\")\n    plt.title(title or f\"Distribution of {column}\")\n    if y_log:\n        plt.yscale(\"log\")\n    plt.tight_layout()\n    if output_path:\n        plt.savefig(output_path)\n    else:\n        plt.show()\n</code></pre>"},{"location":"api/#stroke_predictor.utils.plotting.plot_correlation_heatmap","title":"<code>plot_correlation_heatmap(df, title='Correlation Heatmap of Continuous Variables', figsize=(10, 8), cmap='coolwarm', output_path=None)</code>","text":"<p>Plot a heatmap showing the correlation matrix between continuous (numeric) variables.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>title</code> <code>str</code> <p>Title of the heatmap plot.</p> <code>'Correlation Heatmap of Continuous Variables'</code> <code>figsize</code> <code>tuple of int</code> <p>Size of the figure (width, height).</p> <code>(10, 8)</code> <code>cmap</code> <code>str</code> <p>Colormap to use for the heatmap.</p> <code>'coolwarm'</code> <code>output_path</code> <code>str</code> <p>Path to save the plot image. If None, the plot is shown interactively.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The correlation matrix computed from the numeric columns.</p> Source code in <code>stroke_predictor/utils/plotting.py</code> <pre><code>def plot_correlation_heatmap(\n    df: pd.DataFrame,\n    title: str = \"Correlation Heatmap of Continuous Variables\",\n    figsize: Tuple[int, int] = (10, 8),\n    cmap: str = \"coolwarm\",\n    output_path: Optional[str] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Plot a heatmap showing the correlation matrix between continuous (numeric) variables.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Input DataFrame.\n    title : str, optional\n        Title of the heatmap plot.\n    figsize : tuple of int, optional\n        Size of the figure (width, height).\n    cmap : str, optional\n        Colormap to use for the heatmap.\n    output_path : str, optional\n        Path to save the plot image. If None, the plot is shown interactively.\n\n    Returns\n    -------\n    pd.DataFrame\n        The correlation matrix computed from the numeric columns.\n    \"\"\"\n    numeric_df = df.select_dtypes(include=[\"float64\", \"int64\"])\n    corr_matrix = numeric_df.corr()\n\n    plt.figure(figsize=figsize)\n    sns.heatmap(\n        corr_matrix,\n        annot=True,\n        fmt=\".2f\",\n        cmap=cmap,\n        square=True,\n        cbar_kws={\"shrink\": 0.8},\n    )\n    plt.title(title)\n    plt.tight_layout()\n\n    if output_path:\n        plt.savefig(output_path)\n    else:\n        plt.show()\n\n    return corr_matrix\n</code></pre>"},{"location":"api/#stroke_predictor.utils.plotting.plot_distribution","title":"<code>plot_distribution(df, column, hue=None, stat='count', bins=30, title=None)</code>","text":"<p>Plot the distribution of a continuous variable using seaborn.histplot.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the data.</p> required <code>column</code> <code>str</code> <p>Column name to plot.</p> required <code>hue</code> <code>str</code> <p>Column to use for color encoding.</p> <code>None</code> <code>stat</code> <code>str</code> <p>Statistic to plot ('count', 'density', 'probability'), by default 'count'.</p> <code>'count'</code> <code>bins</code> <code>int</code> <p>Number of histogram bins, by default 30.</p> <code>30</code> <code>title</code> <code>str</code> <p>Title of the plot.</p> <code>None</code> Source code in <code>stroke_predictor/utils/plotting.py</code> <pre><code>def plot_distribution(\n    df: pd.DataFrame,\n    column: str,\n    hue: Optional[str] = None,\n    stat: str = \"count\",\n    bins: int = 30,\n    title: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Plot the distribution of a continuous variable using seaborn.histplot.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        DataFrame containing the data.\n    column : str\n        Column name to plot.\n    hue : str, optional\n        Column to use for color encoding.\n    stat : str, optional\n        Statistic to plot ('count', 'density', 'probability'), by default 'count'.\n    bins : int, optional\n        Number of histogram bins, by default 30.\n    title : str, optional\n        Title of the plot.\n    \"\"\"\n    plt.figure(figsize=(8, 4))\n    sns.histplot(\n        data=df,\n        x=column,\n        hue=hue,\n        stat=stat,\n        bins=bins,\n        element=\"step\",\n        common_norm=False,\n    )\n    plt.title(title or f\"Distribution of {column}\")\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"api/#stroke_predictor.utils.plotting.plot_pairwise","title":"<code>plot_pairwise(df, hue=None, height=2.5)</code>","text":"<p>Create a pairplot for visualizing pairwise relationships in a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with continuous variables.</p> required <code>hue</code> <code>str</code> <p>Column to use for color encoding.</p> <code>None</code> <code>height</code> <code>float</code> <p>Height (in inches) of each facet, by default 2.5.</p> <code>2.5</code> Source code in <code>stroke_predictor/utils/plotting.py</code> <pre><code>def plot_pairwise(\n    df: pd.DataFrame, hue: Optional[str] = None, height: float = 2.5\n) -&gt; None:\n    \"\"\"\n    Create a pairplot for visualizing pairwise relationships in a dataset.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        DataFrame with continuous variables.\n    hue : str, optional\n        Column to use for color encoding.\n    height : float, optional\n        Height (in inches) of each facet, by default 2.5.\n    \"\"\"\n    sns.pairplot(df, hue=hue, height=height)\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"api/#stroke_predictor.utils.plotting.plot_variable_distributions","title":"<code>plot_variable_distributions(series_list, labels, colors, alpha_list=None, bins=30, kde=True, xlabel=None, ylabel='Normalized counts', title=None, output_path=None, stat='density')</code>","text":"<p>Plot overlaid distributions of several variable series using histograms with optional KDE.</p> <p>Parameters:</p> Name Type Description Default <code>series_list</code> <code>list of pd.Series</code> <p>List of pandas Series to plot.</p> required <code>labels</code> <code>list of str</code> <p>Labels for each series, used in the legend.</p> required <code>colors</code> <code>list of str</code> <p>Colors to use for each series.</p> required <code>alpha_list</code> <code>list of float</code> <p>Transparency levels for each histogram. Defaults to 0.5 for all if not specified.</p> <code>None</code> <code>bins</code> <code>int</code> <p>Binning strategy for histograms.</p> <code>30</code> <code>kde</code> <code>bool</code> <p>Whether to overlay a KDE curve. Default is True.</p> <code>True</code> <code>xlabel</code> <code>str</code> <p>Label for the x-axis.</p> <code>None</code> <code>ylabel</code> <code>str</code> <p>Label for the y-axis. Default is \"Normalized counts\".</p> <code>'Normalized counts'</code> <code>title</code> <code>str</code> <p>Plot title.</p> <code>None</code> <code>output_path</code> <code>str</code> <p>Path to save the plot. If None, shows the plot interactively.</p> <code>None</code> <code>stat</code> <code>str</code> <p>Normalization strategy. Options: \"count\", \"frequency\", \"density\", \"probability\". Default is \"density\".</p> <code>'density'</code> Source code in <code>stroke_predictor/utils/plotting.py</code> <pre><code>def plot_variable_distributions(\n    series_list: List[pd.Series],\n    labels: List[str],\n    colors: List[str],\n    alpha_list: Optional[List[float]] = None,\n    bins: Optional[int] = 30,\n    kde: bool = True,\n    xlabel: Optional[str] = None,\n    ylabel: str = \"Normalized counts\",\n    title: Optional[str] = None,\n    output_path: Optional[str] = None,\n    stat: str = \"density\",\n) -&gt; None:\n    \"\"\"\n    Plot overlaid distributions of several variable series using histograms with optional KDE.\n\n    Parameters\n    ----------\n    series_list : list of pd.Series\n        List of pandas Series to plot.\n    labels : list of str\n        Labels for each series, used in the legend.\n    colors : list of str\n        Colors to use for each series.\n    alpha_list : list of float, optional\n        Transparency levels for each histogram. Defaults to 0.5 for all if not specified.\n    bins : int\n        Binning strategy for histograms.\n    kde : bool, optional\n        Whether to overlay a KDE curve. Default is True.\n    xlabel : str, optional\n        Label for the x-axis.\n    ylabel : str, optional\n        Label for the y-axis. Default is \"Normalized counts\".\n    title : str, optional\n        Plot title.\n    output_path : str, optional\n        Path to save the plot. If None, shows the plot interactively.\n    stat : str, optional\n        Normalization strategy. Options: \"count\", \"frequency\", \"density\", \"probability\".\n        Default is \"density\".\n    \"\"\"\n    if alpha_list is None:\n        alpha_list = [0.5] * len(series_list)\n\n    plt.figure(figsize=(10, 6))\n\n    # Compute unified bins if bins is a string like \"auto\" or a single int\n    if isinstance(bins, (int, str)):\n        combined = pd.concat(series_list)\n        min_val, max_val = combined.min(), combined.max()\n        bins_range = np.linspace(min_val, max_val, bins)\n\n    for series, label, color, alpha in zip(series_list, labels, colors, alpha_list):\n        sns.histplot(\n            series.dropna(),\n            kde=kde,\n            bins=bins_range,\n            color=color,\n            alpha=alpha,\n            stat=stat,\n            label=label,\n        )\n\n    plt.xlabel(xlabel or \"Variable\")\n    plt.ylabel(ylabel)\n    if title:\n        plt.title(title)\n    plt.legend()\n    plt.tight_layout()\n\n    if output_path:\n        plt.savefig(output_path)\n    else:\n        plt.show()\n</code></pre>"},{"location":"api/#stroke_predictorutilspreprocessing","title":"\ud83d\udcc2 <code>stroke_predictor.utils.preprocessing</code>","text":""},{"location":"api/#stroke_predictor.utils.preprocessing.categorize_risk","title":"<code>categorize_risk(score)</code>","text":"<p>Categorize health score into risk levels.</p> <p>Parameters:</p> Name Type Description Default <code>score</code> <code>int</code> <p>The computed health score.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The risk category based on the health score.</p> Source code in <code>stroke_predictor/utils/preprocessing.py</code> <pre><code>def categorize_risk(score: int) -&gt; str:\n    \"\"\"\n    Categorize health score into risk levels.\n\n    Parameters\n    ----------\n    score : int\n        The computed health score.\n\n    Returns\n    -------\n    str\n        The risk category based on the health score.\"\"\"\n    if not isinstance(score, int):\n        raise ValueError(\"Score must be an integer.\")\n    if score &lt; 0:\n        raise ValueError(\"Score cannot be negative.\")\n    # Define risk categories based on the score\n    if score &lt;= 1:\n        return \"low risk\"\n    elif score &lt;= 3:\n        return \"moderate risk\"\n    else:\n        return \"high risk\"\n</code></pre>"},{"location":"api/#stroke_predictor.utils.preprocessing.compute_health_score","title":"<code>compute_health_score(row)</code>","text":"<p>Compute a health score based on various health indicators.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Series</code> <p>A row of the DataFrame containing patient data.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The computed health score.</p> Source code in <code>stroke_predictor/utils/preprocessing.py</code> <pre><code>def compute_health_score(row: pd.Series) -&gt; int:\n    \"\"\"\n    Compute a health score based on various health indicators.\n\n    Parameters\n    ----------\n    row : pd.Series\n        A row of the DataFrame containing patient data.\n\n    Returns\n    -------\n    int\n        The computed health score.\n    \"\"\"\n    if not isinstance(row, pd.Series):\n        raise ValueError(\n            \"Input must be a pandas Series representing a row of the DataFrame.\"\n        )\n    if (\n        \"age\" not in row\n        or \"avg_glucose_level\" not in row\n        or \"smoking_status\" not in row\n    ):\n        raise ValueError(\n            \"Row must contain 'age', 'avg_glucose_level', and 'smoking_status' columns.\"\n        )\n    if not isinstance(row[\"age\"], (int, float)) or not isinstance(\n        row[\"avg_glucose_level\"], (int, float)\n    ):\n        raise ValueError(\"'age' and 'avg_glucose_level' must be numeric values.\")\n    if not isinstance(row[\"smoking_status\"], str):\n        raise ValueError(\"'smoking_status' must be a string.\")\n    # Initialize score based on health indicators\n    score = 0\n    score += int(row[\"age\"] &gt; 60)\n    score += int(row[\"avg_glucose_level\"] &gt;= 200)\n    score += int(row[\"smoking_status\"] == \"smokes\")\n    score += int(row[\"hypertension\"] == 1)\n    score += int(row[\"heart_disease\"] == 1)\n    return score\n</code></pre>"},{"location":"api/#stroke_predictor.utils.preprocessing.drop_rows_with_missing","title":"<code>drop_rows_with_missing(df, column, missing_value)</code>","text":"<p>Drop rows with missing values in specified columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>column</code> <code>str</code> <p>Column name to check for missing values.</p> required <code>missing_value</code> <code>Any</code> <p>Value to consider as missing (e.g., np.nan).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with rows containing missing values in specified columns removed.</p> Source code in <code>stroke_predictor/utils/preprocessing.py</code> <pre><code>def drop_rows_with_missing(\n    df: pd.DataFrame, column: str, missing_value: Any\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Drop rows with missing values in specified columns.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Input DataFrame.\n    column : str\n        Column name to check for missing values.\n    missing_value : Any\n        Value to consider as missing (e.g., np.nan).\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with rows containing missing values in specified columns removed.\n    \"\"\"\n    if column not in df.columns:\n        raise ValueError(f\"Column '{column}' not found in DataFrame.\")\n\n    # Drop rows where the specified column has the missing value\n    df_dropped = df[df[column] != missing_value].copy()\n    return df_dropped\n</code></pre>"},{"location":"api/#stroke_predictor.utils.preprocessing.encode_categorical","title":"<code>encode_categorical(df, columns)</code>","text":"<p>Apply Ordinal Encoding to specified columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>columns</code> <code>list</code> <p>List of categorical columns to encode.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with encoded columns.</p> <code>OrdinalEncoder</code> <p>Fitted encoder object.</p> Source code in <code>stroke_predictor/utils/preprocessing.py</code> <pre><code>def encode_categorical(\n    df: pd.DataFrame, columns: List[str]\n) -&gt; Tuple[pd.DataFrame, OrdinalEncoder]:\n    \"\"\"\n    Apply Ordinal Encoding to specified columns.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Input DataFrame.\n    columns : list\n        List of categorical columns to encode.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with encoded columns.\n    OrdinalEncoder\n        Fitted encoder object.\n    \"\"\"\n    if not all(col in df.columns for col in columns):\n        missing_cols = [col for col in columns if col not in df.columns]\n        raise ValueError(f\"Columns {missing_cols} not found in DataFrame.\")\n\n    encoder = OrdinalEncoder()\n    df_encoded = df.copy()\n    df_encoded[columns] = encoder.fit_transform(df[columns])\n    return df_encoded, encoder\n</code></pre>"},{"location":"api/#stroke_predictor.utils.preprocessing.impute_missing_knn","title":"<code>impute_missing_knn(df, columns, n_neighbors=5)</code>","text":"<p>Impute missing values using KNN imputation.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>columns</code> <code>list</code> <p>Columns with missing values to impute.</p> required <code>n_neighbors</code> <code>int</code> <p>Number of neighbors, by default 5.</p> <code>5</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with imputed values.</p> Source code in <code>stroke_predictor/utils/preprocessing.py</code> <pre><code>def impute_missing_knn(\n    df: pd.DataFrame, columns: List[str], n_neighbors: int = 5\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Impute missing values using KNN imputation.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Input DataFrame.\n    columns : list\n        Columns with missing values to impute.\n    n_neighbors : int, optional\n        Number of neighbors, by default 5.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with imputed values.\n    \"\"\"\n    if not all(col in df.columns for col in columns):\n        missing_cols = [col for col in columns if col not in df.columns]\n        raise ValueError(f\"Columns {missing_cols} not found in DataFrame.\")\n\n    imputer = KNNImputer(n_neighbors=n_neighbors)\n    df_imputed = df.copy()\n    df_imputed[columns] = imputer.fit_transform(df[columns])\n    return df_imputed\n</code></pre>"},{"location":"api/#stroke_predictor.utils.preprocessing.preprocess_dataframe","title":"<code>preprocess_dataframe(df, columns, variable_encoder)</code>","text":"<p>Preprocess the DataFrame by creating engineered features, computing health scores, and encoding categorical variables.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing patient data.</p> required <code>columns</code> <code>list</code> <p>The list of columns to include in the final DataFrame.</p> required <code>variable_encoder</code> <code>OrdinalEncoder</code> <p>The fitted OrdinalEncoder to transform categorical variables.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The preprocessed DataFrame with engineered features and encoded categorical variables.</p> Source code in <code>stroke_predictor/utils/preprocessing.py</code> <pre><code>def preprocess_dataframe(\n    df: pd.DataFrame,\n    columns: list,\n    variable_encoder: sklearn.preprocessing.OrdinalEncoder,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Preprocess the DataFrame by creating engineered features, computing health scores, and encoding categorical variables.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        The input DataFrame containing patient data.\n    columns : list\n        The list of columns to include in the final DataFrame.\n    variable_encoder : sklearn.preprocessing.OrdinalEncoder\n        The fitted OrdinalEncoder to transform categorical variables.\n\n    Returns\n    -------\n    pd.DataFrame\n        The preprocessed DataFrame with engineered features and encoded categorical variables.\n    \"\"\"\n    # Check if required columns are present\n    required_columns = [\n        \"age\",\n        \"avg_glucose_level\",\n        \"bmi\",\n        \"hypertension\",\n        \"heart_disease\",\n        \"smoking_status\",\n    ]\n    if not all(col in df.columns for col in required_columns):\n        missing_cols = [col for col in required_columns if col not in df.columns]\n        raise ValueError(f\"Missing required columns: {missing_cols}\")\n    # Check if the variable_encoder is fitted\n    if not hasattr(variable_encoder, \"categories_\"):\n        raise ValueError(\"The variable_encoder must be a fitted OrdinalEncoder.\")\n    # Ensure the DataFrame has the correct data types\n    for col in required_columns:\n        if col not in df.columns:\n            raise ValueError(\n                f\"Column '{col}' is required but not found in the DataFrame.\"\n            )\n        if col == \"smoking_status\":\n            df[col] = df[col].astype(\"category\")\n        else:\n            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n    # Create the engineered features\n    df[\"bmi_category\"] = pd.cut(\n        df[\"bmi\"],\n        bins=[0, 18.5, 25, 30, 100],\n        labels=[\"underweight\", \"normal\", \"overweight\", \"obese\"],\n    )\n    df[\"age_group\"] = pd.cut(\n        df[\"age\"],\n        bins=[0, 30, 45, 60, 75, 100],\n        labels=[\"young\", \"adult\", \"middle_aged\", \"senior\", \"elderly\"],\n    )\n    df[\"glucose_category\"] = pd.cut(\n        df[\"avg_glucose_level\"],\n        bins=[0, 99, 125, 300],\n        labels=[\"normal\", \"prediabetic\", \"diabetic\"],\n    )\n    # Compute health score for each individual\n    df[\"health_score\"] = df.apply(compute_health_score, axis=1)\n    # Categorize score into risk levels\n    df[\"risk_category\"] = df[\"health_score\"].apply(categorize_risk)\n    # Apply the loaded encoder\n    categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns\n    print(categorical_cols)\n    df_encoded = df.copy()\n    df_encoded[categorical_cols] = variable_encoder.transform(df[categorical_cols])\n    # Reorder columns to match the original DataFrame\n    df_encoded = df_encoded[columns]\n    return df_encoded\n</code></pre>"},{"location":"api/#stroke_predictor.utils.preprocessing.replace_missing_to_value","title":"<code>replace_missing_to_value(df, column, missing_value, new_value)</code>","text":"<p>Replace missing values in specified columns with a given value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>column</code> <code>str</code> <p>Column name to process.</p> required <code>missing_value</code> <code>Any</code> <p>Value to consider as missing (e.g., np.nan).</p> required <code>new_value</code> <code>Any</code> <p>Value to replace missing values with.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with specified missing values replaced.</p> Source code in <code>stroke_predictor/utils/preprocessing.py</code> <pre><code>def replace_missing_to_value(\n    df: pd.DataFrame, column: str, missing_value: Any, new_value: Any\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Replace missing values in specified columns with a given value.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Input DataFrame.\n    column : str\n        Column name to process.\n    missing_value : Any\n        Value to consider as missing (e.g., np.nan).\n    new_value : Any\n        Value to replace missing values with.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with specified missing values replaced.\n    \"\"\"\n    if column not in df.columns:\n        raise ValueError(f\"Column '{column}' not found in DataFrame.\")\n\n    # Replace missing values in the specified column\n    df_replaced = df.copy()\n    df_replaced[column] = df_replaced[column].replace(missing_value, new_value)\n    return df_replaced\n</code></pre>"},{"location":"api/#stroke_predictor.utils.preprocessing.split_and_rebuild_dataframe","title":"<code>split_and_rebuild_dataframe(df, target_column, test_size=0.3, random_state=42, stratify=True)</code>","text":"<p>Split a DataFrame into train and test sets, and return both with the target column included.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame including the target column.</p> required <code>target_column</code> <code>str</code> <p>The name of the target column.</p> required <code>test_size</code> <code>float</code> <p>Fraction of the dataset to include in the test split. Default is 0.3.</p> <code>0.3</code> <code>random_state</code> <code>int</code> <p>Random state for reproducibility. Default is 42.</p> <code>42</code> <code>stratify</code> <code>bool</code> <p>Whether to stratify based on the target column. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame]</code> <p>Training and test sets including the target column.</p> Source code in <code>stroke_predictor/utils/preprocessing.py</code> <pre><code>def split_and_rebuild_dataframe(\n    df: pd.DataFrame,\n    target_column: str,\n    test_size: float = 0.3,\n    random_state: int = 42,\n    stratify: bool = True,\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Split a DataFrame into train and test sets, and return both with the target column included.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        The input DataFrame including the target column.\n    target_column : str\n        The name of the target column.\n    test_size : float, optional\n        Fraction of the dataset to include in the test split. Default is 0.3.\n    random_state : int, optional\n        Random state for reproducibility. Default is 42.\n    stratify : bool, optional\n        Whether to stratify based on the target column. Default is True.\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, pd.DataFrame]\n        Training and test sets including the target column.\n    \"\"\"\n    if target_column not in df.columns:\n        raise ValueError(f\"Target column '{target_column}' not found in DataFrame.\")\n\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X,\n        y,\n        test_size=test_size,\n        random_state=random_state,\n        stratify=y if stratify else None,\n    )\n\n    df_train = X_train.copy()\n    df_train[target_column] = y_train\n\n    df_test = X_test.copy()\n    df_test[target_column] = y_test\n\n    # Ensure columns are in the same order as original DataFrame\n    df_train = df_train[df.columns]\n    df_test = df_test[df.columns]\n\n    return df_train, df_test\n</code></pre>"},{"location":"api/#stroke_predictoroptuna_optimization","title":"\ud83d\udcc2 <code>stroke_predictor.optuna_optimization</code>","text":""},{"location":"api/#stroke_predictor.optuna_optimization.runner.run_optimization","title":"<code>run_optimization(config_path)</code>","text":"<p>Optimize the stroke prediction model using Optuna.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the configuration file containing model and optimization parameters.</p> required <p>Returns:</p> Type Description <code>Study</code> <p>The Optuna study object containing the optimization results.</p> Source code in <code>stroke_predictor/optuna_optimization/runner.py</code> <pre><code>def run_optimization(config_path: str) -&gt; optuna.Study:\n    \"\"\"\n    Optimize the stroke prediction model using Optuna.\n\n    Parameters\n    ----------\n    config_path : str\n        Path to the configuration file containing model and optimization parameters.\n\n    Returns\n    -------\n    optuna.Study\n        The Optuna study object containing the optimization results.\n    \"\"\"\n\n    with open(config_path, \"r\") as f:\n        config = yaml.safe_load(f)\n\n    global_config = config[\"global\"]\n    data_config = config[\"storage\"]\n    model_configs = config[\"models\"]\n    optuna_config = config[\"optuna\"]\n\n    # Load data\n    X, y = load_dataset(\n        path=Path(data_config[\"data_path\"]),\n        key=data_config[\"hdf5_key_train\"],\n        target=data_config[\"target\"],\n    )\n\n    # Apply SMOTE to handle class imbalance\n    smote = SMOTE(random_state=global_config[\"seed\"])\n    X, y = smote.fit_resample(X, y)\n\n    # Set up MLflow experiment\n    mlflow.set_experiment(optuna_config[\"study_name\"])\n\n    sampler = getattr(optuna.samplers, optuna_config[\"sampler\"])(\n        seed=global_config[\"seed\"]\n    )\n    pruner = getattr(optuna.pruners, optuna_config[\"pruner\"])()\n\n    study = optuna.create_study(\n        study_name=optuna_config[\"study_name\"],\n        direction=global_config[\"direction\"],\n        sampler=sampler,\n        pruner=pruner,\n        storage=optuna_config[\"storage\"],\n        load_if_exists=True,\n    )\n\n    study.optimize(\n        lambda trial: objective(trial, X, y, model_configs, global_config, data_config),\n        n_trials=optuna_config[\"n_trials\"],\n        timeout=optuna_config.get(\"timeout\", None),\n        catch=(RuntimeError, TypeError, ValueError),\n    )\n\n    print(\"Best trial:\")\n    print(study.best_trial)\n\n    # -------------------------------------------------------------------------\n    # Results\n    # -------------------------------------------------------------------------\n\n    # Find number of pruned and completed trials\n    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n\n    # Display the study statistics\n    print(\"\\nStudy statistics: \")\n    print(\"  Number of finished trials: \", len(study.trials))\n    print(\"  Number of pruned trials: \", len(pruned_trials))\n    print(\"  Number of complete trials: \", len(complete_trials))\n\n    trial = study.best_trial\n    print(\"Best trial:\")\n    print(\"  Value: \", trial.value)\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))\n\n    # Save results to csv file\n    df = study.trials_dataframe()\n    df = df.loc[df[\"state\"] == \"COMPLETE\"]  # Keep only results that did not prune\n    df = df.drop(\"state\", axis=1)  # Exclude state column\n    df = df.sort_values(\"value\")  # Sort based on accuracy\n    df.to_csv(\n        Path(optuna_config[\"output_csv\"]),\n        index=False,\n    )  # Save to csv file\n\n    # Display results in a dataframe\n    print(\"\\nOverall Results (ordered by accuracy):\\n {}\".format(df))\n\n    # Find the most important hyperparameters\n    most_important_parameters = optuna.importance.get_param_importances(\n        study, target=None\n    )\n\n    # Display the most important hyperparameters\n    print(\"\\nMost important hyperparameters:\")\n    for key, value in most_important_parameters.items():\n        print(\"  {}:{}{:.2f}%\".format(key, (15 - len(key)) * \" \", value * 100))\n\n    return study\n</code></pre>"},{"location":"api/#stroke_predictor.optuna_optimization.runner.train_algorithm","title":"<code>train_algorithm(config_path, study)</code>","text":"<p>Train the best model based on the Optuna study results.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the configuration file.</p> required <code>study</code> <code>Study</code> <p>The Optuna study containing the best trial parameters.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>stroke_predictor/optuna_optimization/runner.py</code> <pre><code>def train_algorithm(config_path: str, study: optuna.Study) -&gt; None:\n    \"\"\"\n    Train the best model based on the Optuna study results.\n\n    Parameters\n    ----------\n    config_path : str\n        Path to the configuration file.\n    study : optuna.Study\n        The Optuna study containing the best trial parameters.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    # Load configuration\n    with open(config_path, \"r\") as f:\n        config = yaml.safe_load(f)\n\n    # Load its sections\n    global_config = config[\"global\"]\n    data_config = config[\"storage\"]\n\n    # Load Optuna study\n    if not isinstance(study, optuna.study.Study):\n        study = optuna.load_study(\n            study_name=config[\"optuna\"][\"study_name\"],\n            storage=config[\"optuna\"][\"storage\"],\n        )\n\n    # Get the best parameters from the study\n    best_params = study.best_trial.params\n    classifier_name = best_params[\"classifier\"]\n\n    print(f\"Best parameters for {classifier_name}: {best_params}\")\n\n    # Load data\n    X_train, y_train = load_dataset(\n        path=Path(data_config[\"data_path\"]),\n        key=data_config[\"hdf5_key_train\"],\n        target=data_config[\"target\"],\n    )\n\n    # Apply SMOTE to handle class imbalance\n    smote = SMOTE(random_state=global_config[\"seed\"])\n    X_train, y_train = smote.fit_resample(X_train, y_train)\n\n    # Build and train model\n    model = get_model(classifier_name, best_params, seed=global_config[\"seed\"])\n    pipeline = Pipeline(\n        [\n            (\"scaler\", StandardScaler()),\n            (\"clf\", model),\n        ]\n    )\n    pipeline.fit(X_train, y_train)\n\n    # Save model with MLflow\n    output_path = Path(config[\"storage\"][\"model_output\"])\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n\n    with mlflow.start_run():\n        # Log model\n        mlflow.sklearn.log_model(\n            pipeline, name=classifier_name, input_example=X_train[:1]\n        )\n\n        # Log params or metrics\n        mlflow.log_params(best_params)\n        mlflow.log_param(\"classifier\", classifier_name)\n\n        # Save the model locally as well\n        mlflow.sklearn.save_model(pipeline, output_path)\n\n        print(f\"Model saved to {output_path}\")\n</code></pre>"},{"location":"api/#stroke_predictorconfigs","title":"\ud83d\udcc2 <code>stroke_predictor.configs</code>","text":"<p>This directory contains YAML configuration files and is not directly documented in the API. Configuration examples can be found in the main README or training notebooks.</p>"},{"location":"api/#stroke_predictormain","title":"\ud83d\udcc2 <code>stroke_predictor.main</code>","text":""},{"location":"api/#stroke_predictor.main.get_form","title":"<code>get_form(request)</code>  <code>async</code>","text":"<p>Render the input form for stroke prediction.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>The FastAPI request object.</p> required <p>Returns:</p> Type Description <code>HTMLResponse</code> <p>The HTML response containing the input form.</p> Source code in <code>stroke_predictor/main.py</code> <pre><code>@app.get(\"/web\", response_class=HTMLResponse)  # noqa\nasync def get_form(request: Request) -&gt; HTMLResponse:\n    \"\"\"\n    Render the input form for stroke prediction.\n\n    Parameters\n    ----------\n    request : Request\n        The FastAPI request object.\n\n    Returns\n    -------\n    HTMLResponse\n        The HTML response containing the input form.\"\"\"\n    return templates.TemplateResponse(\"form.html\", {\"request\": request, \"result\": None})\n</code></pre>"},{"location":"api/#stroke_predictor.main.predict","title":"<code>predict(features)</code>","text":"<p>Predict stroke risk based on patient features.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>BaseModel</code> <p>The patient features for stroke prediction.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the stroke prediction and its probability.</p> Source code in <code>stroke_predictor/main.py</code> <pre><code>@app.post(\"/predict\")  # noqa\ndef predict(features: PatientFeatures) -&gt; dict:\n    \"\"\"\n    Predict stroke risk based on patient features.\n\n    Parameters\n    ----------\n    features : BaseModel\n        The patient features for stroke prediction.\n\n    Returns\n    -------\n    dict\n        A dictionary containing the stroke prediction and its probability.\n    \"\"\"\n    # Convert input to DataFrame\n    input_df = pd.DataFrame([features.dict()])\n\n    # Preprocess the input data\n    input_df = preprocess_dataframe(\n        input_df, columns=column_names, variable_encoder=variable_encoder\n    )\n\n    print(f\"Processed input data:\\n{input_df}\")\n\n    # Run prediction using helper function\n    prediction, probability = make_prediction(input_df, model)\n\n    return {\n        \"stroke_prediction\": prediction,\n        \"stroke_probability\": round(probability, 4),\n    }\n</code></pre>"},{"location":"api/#stroke_predictor.main.predict_form","title":"<code>predict_form(request, age=Form(...), hypertension=Form(...), heart_disease=Form(...), avg_glucose_level=Form(...), bmi=Form(...), gender=Form(...), ever_married=Form(...), work_type=Form(...), Residence_type=Form(...), smoking_status=Form(...))</code>  <code>async</code>","text":"<p>Handle form submission for stroke prediction.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>The FastAPI request object.</p> required <code>age</code> <code>float</code> <p>Age of the patient.</p> <code>Form(...)</code> <code>hypertension</code> <code>int</code> <p>Hypertension status (0 or 1).</p> <code>Form(...)</code> <code>heart_disease</code> <code>int</code> <p>Heart disease status (0 or 1).</p> <code>Form(...)</code> <code>avg_glucose_level</code> <code>float</code> <p>Average glucose level of the patient.</p> <code>Form(...)</code> <code>bmi</code> <code>float</code> <p>Body Mass Index of the patient.</p> <code>Form(...)</code> <code>gender</code> <code>Literal[\"Male\", \"Female\", \"Other\"]:</code> <p>Gender of the patient.</p> <code>Form(...)</code> <code>ever_married</code> <code>Literal[Yes, No]</code> <p>Marital status of the patient.</p> <code>Form(...)</code> <code>work_type</code> <code>Literal[Private, Self - employed, Govt_job, children, Never_worked]</code> <p>Type of work the patient is engaged in.</p> <code>Form(...)</code> <code>Residence_type</code> <code>Literal[Urban, Rural]</code> <p>Type of residence (Urban or Rural).</p> <code>Form(...)</code> <code>smoking_status</code> <code>Literal['formerly smoked', 'never smoked', smokes, Unknown]</code> <p>Smoking status of the patient.</p> <code>Form(...)</code> <p>Returns:</p> Type Description <code>HTMLResponse</code> <p>The HTML response containing the prediction result and input values.</p> Source code in <code>stroke_predictor/main.py</code> <pre><code>@app.post(\"/web\", response_class=HTMLResponse)  # noqa\nasync def predict_form(\n    request: Request,\n    age: float = Form(...),\n    hypertension: int = Form(...),\n    heart_disease: int = Form(...),\n    avg_glucose_level: float = Form(...),\n    bmi: float = Form(...),\n    gender: str = Form(...),\n    ever_married: str = Form(...),\n    work_type: str = Form(...),\n    Residence_type: str = Form(...),\n    smoking_status: str = Form(...),\n) -&gt; HTMLResponse:\n    \"\"\"\n    Handle form submission for stroke prediction.\n\n    Parameters\n    ----------\n    request : Request\n        The FastAPI request object.\n    age : float\n        Age of the patient.\n    hypertension : int\n        Hypertension status (0 or 1).\n    heart_disease : int\n        Heart disease status (0 or 1).\n    avg_glucose_level : float\n        Average glucose level of the patient.\n    bmi : float\n        Body Mass Index of the patient.\n    gender : Literal[\"Male\", \"Female\", \"Other\"]:\n        Gender of the patient.\n    ever_married : Literal[\"Yes\", \"No\"]\n        Marital status of the patient.\n    work_type : Literal[\"Private\", \"Self-employed\", \"Govt_job\", \"children\", \"Never_worked\"]\n        Type of work the patient is engaged in.\n    Residence_type : Literal[\"Urban\", \"Rural\"]\n        Type of residence (Urban or Rural).\n    smoking_status : Literal[\"formerly smoked\", \"never smoked\", \"smokes\", \"Unknown\"]\n        Smoking status of the patient.\n\n    Returns\n    -------\n    HTMLResponse\n        The HTML response containing the prediction result and input values.\n    \"\"\"\n    # Build input DataFrame\n    input_data = pd.DataFrame(\n        [\n            {\n                \"age\": age,\n                \"hypertension\": hypertension,\n                \"heart_disease\": heart_disease,\n                \"avg_glucose_level\": avg_glucose_level,\n                \"bmi\": bmi,\n                \"gender\": gender,\n                \"ever_married\": ever_married,\n                \"work_type\": work_type,\n                \"Residence_type\": Residence_type,\n                \"smoking_status\": smoking_status,\n            }\n        ]\n    )\n\n    # Preprocess\n    processed_input = preprocess_dataframe(\n        input_data, columns=column_names, variable_encoder=variable_encoder\n    )\n    print(f\"Processed input data:\\n{processed_input}\")\n    prediction, probability = make_prediction(processed_input, model)\n\n    if prediction == 1:\n        result = f\"The patient is likely to suffer a stroke with a probability of {probability:.2%}.\"\n    else:\n        result = f\"It is unlikely that the patient will suffer a stroke with a probability of {probability:.2%}.\"\n\n    input_values = {\n        \"age\": age,\n        \"hypertension\": hypertension,\n        \"heart_disease\": heart_disease,\n        \"avg_glucose_level\": avg_glucose_level,\n        \"bmi\": bmi,\n        \"gender\": gender,\n        \"ever_married\": ever_married,\n        \"work_type\": work_type,\n        \"Residence_type\": Residence_type,\n        \"smoking_status\": smoking_status,\n    }\n\n    return templates.TemplateResponse(\n        \"form.html\",\n        {\"request\": request, \"result\": result, \"input_values\": input_values},\n    )\n</code></pre>"},{"location":"api/#stroke_predictor.main.read_root","title":"<code>read_root()</code>","text":"<p>Root endpoint to check if the API is running.</p> Source code in <code>stroke_predictor/main.py</code> <pre><code>@app.get(\"/\")  # noqa\ndef read_root() -&gt; dict:\n    \"\"\"Root endpoint to check if the API is running.\"\"\"\n    return {\"message\": \"Stroke prediction model is up and running.\"}\n</code></pre>"}]}