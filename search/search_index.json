{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"BSG Stroke Classifier","text":"<p>Welcome to the documentation for the BSG Stroke Classifier, a machine learning project designed to predict the likelihood of stroke events using patient medical records. This project focuses on the analysis, preprocessing, and modeling of healthcare data to build a reliable binary classifier.</p>"},{"location":"#project-overview","title":"Project Overview","text":"<p>The goal of this project is to explore a healthcare dataset and build an end-to-end classification pipeline for stroke prediction. It includes:</p> <ul> <li>Data cleaning and imputation of missing values.</li> <li>Exploratory Data Analysis (EDA) to understand feature relationships.</li> <li>Preprocessing, including encoding and scaling.</li> <li>Machine learning modeling using classification algorithms.</li> <li>Evaluation metrics to assess performance.</li> <li>A clean and reproducible structure using Python, Poetry, and MkDocs.</li> </ul>"},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>bsg-testdatascience-1/\n\u251c\u2500\u2500 data/ # Raw and processed data files\n\u251c\u2500\u2500 docs/ # Documentation (MkDocs)\n\u251c\u2500\u2500 notebooks/ # EDA and experimentation notebooks\n\u251c\u2500\u2500 stroke_predictor/ # Source code and utility functions\n\u2502 \u251c\u2500\u2500 utils/ # Preprocessing, plotting, and I/O helpers\n\u2502 \u2514\u2500\u2500 tests/ # Unit tests\n\u251c\u2500\u2500 pyproject.toml # Poetry config\n\u251c\u2500\u2500 mkdocs.yml # MkDocs config\n\u2514\u2500\u2500 README.md # Project overview\n</code></pre>"},{"location":"#documentation-contents","title":"Documentation Contents","text":"<ul> <li>Exploratory Data Analysis \u2013 Summary statistics, visualizations, and patterns in the dataset.</li> <li>Preprocessing Utilities \u2013 Code for missing value imputation, encoding, and normalization.</li> <li>Modeling Pipeline \u2013 Train/test splits, model training, and evaluation.</li> <li>Testing \u2013 Unit tests for critical components.</li> <li>API Reference \u2013 Autogenerated from code docstrings using MkDocs + <code>mkdocstrings</code>.</li> </ul>"},{"location":"#how-to-use","title":"How to Use","text":"<ol> <li>Clone the repository:</li> </ol> <pre><code>git clone https://github.com/borja-sg/BSG-TestDataScience-1\ncd BSG-TestDataScience-1\n</code></pre> <p>2.Install dependencies using Poetry:</p> <pre><code>poetry install\n</code></pre> <ol> <li>Run the notebooks or scripts from the root folder.</li> </ol>"},{"location":"api/","title":"\ud83d\udd27 API Reference","text":"<p>This section contains the auto-generated documentation for the main utility modules in the <code>stroke_predictor</code> package.</p>"},{"location":"api/#stroke_predictorutilsdata_io","title":"\ud83d\udcc2 <code>stroke_predictor.utils.data_io</code>","text":""},{"location":"api/#stroke_predictor.utils.data_io.load_csv_dataset","title":"<code>load_csv_dataset(path)</code>","text":"<p>Load a CSV file as a pandas DataFrame. Raises an error if the file does not exist.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the CSV file.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Loaded DataFrame.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified file does not exist.</p> <code>ValueError</code> <p>If the path is not a file (e.g., a directory).</p> Source code in <code>stroke_predictor/utils/data_io.py</code> <pre><code>def load_csv_dataset(path: Path) -&gt; pd.DataFrame:\n    \"\"\"\n    Load a CSV file as a pandas DataFrame. Raises an error if the file does not exist.\n\n    Parameters\n    ----------\n    path : str\n        Path to the CSV file.\n\n    Returns\n    -------\n    pd.DataFrame\n        Loaded DataFrame.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the specified file does not exist.\n    ValueError\n        If the path is not a file (e.g., a directory).\n    \"\"\"\n\n    # Check if the path exists and is a file\n    if not path.exists():\n        raise FileNotFoundError(f\"The file '{path}' does not exist.\")\n    if not path.is_file():\n        raise ValueError(f\"'{path}' is not a file (may be a directory).\")\n\n    # Load and return the DataFrame\n    return pd.read_csv(path)\n</code></pre>"},{"location":"api/#stroke_predictor.utils.data_io.save_to_hdf","title":"<code>save_to_hdf(df, path, key, mode)</code>","text":"<p>Save a DataFrame to an HDF5 file.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to save.</p> required <code>path</code> <code>Path</code> <p>Destination HDF5 file path.</p> required <code>key</code> <code>str</code> <p>Key under which the DataFrame is stored.</p> required <code>mode</code> <code>str</code> <p>Mode to open the file ('w' for write, 'a' for append), by default 'a'.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the mode is not 'w' or 'a'.</p> <code>TypeError</code> <p>If the key is not a string.</p> Source code in <code>stroke_predictor/utils/data_io.py</code> <pre><code>def save_to_hdf(\n    df: pd.DataFrame, path: Path, key: str, mode: Literal[\"a\", \"w\", \"r+\"]\n) -&gt; None:\n    \"\"\"\n    Save a DataFrame to an HDF5 file.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        DataFrame to save.\n    path : Path\n        Destination HDF5 file path.\n    key : str\n        Key under which the DataFrame is stored.\n    mode : str, optional\n        Mode to open the file ('w' for write, 'a' for append), by default 'a'.\n\n    Raises\n    ------\n    ValueError\n        If the mode is not 'w' or 'a'.\n    TypeError\n        If the key is not a string.\n    \"\"\"\n\n    # Create parent directories if they don't exist\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Validate mode\n    if mode not in [\"w\", \"a\", \"r+\"]:\n        raise ValueError(\"Mode must be either 'a' or 'w', 'r+'\")\n\n    # Validate key\n    if not isinstance(key, str):\n        raise TypeError(\"Key must be a string\")\n\n    # Check if the path folder exists\n    if not path.parent.exists():\n        raise FileNotFoundError(f\"The directory '{path.parent}' does not exist.\")\n\n    # Save the DataFrame to HDF5\n    df.to_hdf(path, key=key, mode=mode, format=\"table\")\n</code></pre>"},{"location":"api/#stroke_predictorutilsplotting","title":"\ud83d\udcc2 <code>stroke_predictor.utils.plotting</code>","text":""},{"location":"api/#stroke_predictor.utils.plotting.plot_categorical_counts","title":"<code>plot_categorical_counts(df, column, hue=None, normalize=False, title=None, y_log=False, output_path=None)</code>","text":"<p>Plot the counts of a categorical column using seaborn.countplot.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>column</code> <code>str</code> <p>Column name to count.</p> required <code>hue</code> <code>str</code> <p>Column to use for bar color separation.</p> <code>None</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize counts per group (hue-level).</p> <code>False</code> <code>title</code> <code>str</code> <p>Title of the plot.</p> <code>None</code> <code>output_path</code> <code>str</code> <p>Path to save the plot image. If None, the plot is shown interactively.</p> <code>None</code> Source code in <code>stroke_predictor/utils/plotting.py</code> <pre><code>def plot_categorical_counts(\n    df: pd.DataFrame,\n    column: str,\n    hue: Optional[str] = None,\n    normalize: bool = False,\n    title: Optional[str] = None,\n    y_log: bool = False,\n    output_path: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Plot the counts of a categorical column using seaborn.countplot.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Input DataFrame.\n    column : str\n        Column name to count.\n    hue : str, optional\n        Column to use for bar color separation.\n    normalize : bool, optional\n        Whether to normalize counts per group (hue-level).\n    title : str, optional\n        Title of the plot.\n    output_path : str, optional\n        Path to save the plot image. If None, the plot is shown interactively.\n    \"\"\"\n    plt.figure(figsize=(8, 4))\n    data = df.copy()\n    if normalize and hue:\n        norm_df = (\n            data.groupby([column, hue], observed=False)\n            .size()\n            .div(data.groupby(column, observed=False).size(), level=column)\n            .reset_index(name=\"proportion\")\n        )\n        sns.barplot(data=norm_df, x=column, y=\"proportion\", hue=hue)\n        plt.ylabel(\"Proportion\")\n    else:\n        sns.countplot(data=data, x=column, hue=hue)\n        plt.ylabel(\"Counts\")\n    plt.title(title or f\"Distribution of {column}\")\n    if y_log:\n        plt.yscale(\"log\")\n    plt.tight_layout()\n    if output_path:\n        plt.savefig(output_path)\n    else:\n        plt.show()\n</code></pre>"},{"location":"api/#stroke_predictor.utils.plotting.plot_correlation_heatmap","title":"<code>plot_correlation_heatmap(df, title='Correlation Heatmap of Continuous Variables', figsize=(10, 8), cmap='coolwarm', output_path=None)</code>","text":"<p>Plot a heatmap showing the correlation matrix between continuous (numeric) variables.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>title</code> <code>str</code> <p>Title of the heatmap plot.</p> <code>'Correlation Heatmap of Continuous Variables'</code> <code>figsize</code> <code>tuple of int</code> <p>Size of the figure (width, height).</p> <code>(10, 8)</code> <code>cmap</code> <code>str</code> <p>Colormap to use for the heatmap.</p> <code>'coolwarm'</code> <code>output_path</code> <code>str</code> <p>Path to save the plot image. If None, the plot is shown interactively.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The correlation matrix computed from the numeric columns.</p> Source code in <code>stroke_predictor/utils/plotting.py</code> <pre><code>def plot_correlation_heatmap(\n    df: pd.DataFrame,\n    title: str = \"Correlation Heatmap of Continuous Variables\",\n    figsize: Tuple[int, int] = (10, 8),\n    cmap: str = \"coolwarm\",\n    output_path: Optional[str] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Plot a heatmap showing the correlation matrix between continuous (numeric) variables.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Input DataFrame.\n    title : str, optional\n        Title of the heatmap plot.\n    figsize : tuple of int, optional\n        Size of the figure (width, height).\n    cmap : str, optional\n        Colormap to use for the heatmap.\n    output_path : str, optional\n        Path to save the plot image. If None, the plot is shown interactively.\n\n    Returns\n    -------\n    pd.DataFrame\n        The correlation matrix computed from the numeric columns.\n    \"\"\"\n    numeric_df = df.select_dtypes(include=[\"float64\", \"int64\"])\n    corr_matrix = numeric_df.corr()\n\n    plt.figure(figsize=figsize)\n    sns.heatmap(\n        corr_matrix,\n        annot=True,\n        fmt=\".2f\",\n        cmap=cmap,\n        square=True,\n        cbar_kws={\"shrink\": 0.8},\n    )\n    plt.title(title)\n    plt.tight_layout()\n\n    if output_path:\n        plt.savefig(output_path)\n    else:\n        plt.show()\n\n    return corr_matrix\n</code></pre>"},{"location":"api/#stroke_predictor.utils.plotting.plot_distribution","title":"<code>plot_distribution(df, column, hue=None, stat='count', bins=30, title=None)</code>","text":"<p>Plot the distribution of a continuous variable using seaborn.histplot.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the data.</p> required <code>column</code> <code>str</code> <p>Column name to plot.</p> required <code>hue</code> <code>str</code> <p>Column to use for color encoding.</p> <code>None</code> <code>stat</code> <code>str</code> <p>Statistic to plot ('count', 'density', 'probability'), by default 'count'.</p> <code>'count'</code> <code>bins</code> <code>int</code> <p>Number of histogram bins, by default 30.</p> <code>30</code> <code>title</code> <code>str</code> <p>Title of the plot.</p> <code>None</code> Source code in <code>stroke_predictor/utils/plotting.py</code> <pre><code>def plot_distribution(\n    df: pd.DataFrame,\n    column: str,\n    hue: Optional[str] = None,\n    stat: str = \"count\",\n    bins: int = 30,\n    title: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Plot the distribution of a continuous variable using seaborn.histplot.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        DataFrame containing the data.\n    column : str\n        Column name to plot.\n    hue : str, optional\n        Column to use for color encoding.\n    stat : str, optional\n        Statistic to plot ('count', 'density', 'probability'), by default 'count'.\n    bins : int, optional\n        Number of histogram bins, by default 30.\n    title : str, optional\n        Title of the plot.\n    \"\"\"\n    plt.figure(figsize=(8, 4))\n    sns.histplot(\n        data=df,\n        x=column,\n        hue=hue,\n        stat=stat,\n        bins=bins,\n        element=\"step\",\n        common_norm=False,\n    )\n    plt.title(title or f\"Distribution of {column}\")\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"api/#stroke_predictor.utils.plotting.plot_pairwise","title":"<code>plot_pairwise(df, hue=None, height=2.5)</code>","text":"<p>Create a pairplot for visualizing pairwise relationships in a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with continuous variables.</p> required <code>hue</code> <code>str</code> <p>Column to use for color encoding.</p> <code>None</code> <code>height</code> <code>float</code> <p>Height (in inches) of each facet, by default 2.5.</p> <code>2.5</code> Source code in <code>stroke_predictor/utils/plotting.py</code> <pre><code>def plot_pairwise(\n    df: pd.DataFrame, hue: Optional[str] = None, height: float = 2.5\n) -&gt; None:\n    \"\"\"\n    Create a pairplot for visualizing pairwise relationships in a dataset.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        DataFrame with continuous variables.\n    hue : str, optional\n        Column to use for color encoding.\n    height : float, optional\n        Height (in inches) of each facet, by default 2.5.\n    \"\"\"\n    sns.pairplot(df, hue=hue, height=height)\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"api/#stroke_predictor.utils.plotting.plot_variable_distributions","title":"<code>plot_variable_distributions(series_list, labels, colors, alpha_list=None, bins=30, kde=True, xlabel=None, ylabel='Normalized counts', title=None, output_path=None, stat='density')</code>","text":"<p>Plot overlaid distributions of several variable series using histograms with optional KDE.</p> <p>Parameters:</p> Name Type Description Default <code>series_list</code> <code>list of pd.Series</code> <p>List of pandas Series to plot.</p> required <code>labels</code> <code>list of str</code> <p>Labels for each series, used in the legend.</p> required <code>colors</code> <code>list of str</code> <p>Colors to use for each series.</p> required <code>alpha_list</code> <code>list of float</code> <p>Transparency levels for each histogram. Defaults to 0.5 for all if not specified.</p> <code>None</code> <code>bins</code> <code>int</code> <p>Binning strategy for histograms.</p> <code>30</code> <code>kde</code> <code>bool</code> <p>Whether to overlay a KDE curve. Default is True.</p> <code>True</code> <code>xlabel</code> <code>str</code> <p>Label for the x-axis.</p> <code>None</code> <code>ylabel</code> <code>str</code> <p>Label for the y-axis. Default is \"Normalized counts\".</p> <code>'Normalized counts'</code> <code>title</code> <code>str</code> <p>Plot title.</p> <code>None</code> <code>output_path</code> <code>str</code> <p>Path to save the plot. If None, shows the plot interactively.</p> <code>None</code> <code>stat</code> <code>str</code> <p>Normalization strategy. Options: \"count\", \"frequency\", \"density\", \"probability\". Default is \"density\".</p> <code>'density'</code> Source code in <code>stroke_predictor/utils/plotting.py</code> <pre><code>def plot_variable_distributions(\n    series_list: List[pd.Series],\n    labels: List[str],\n    colors: List[str],\n    alpha_list: Optional[List[float]] = None,\n    bins: Optional[int] = 30,\n    kde: bool = True,\n    xlabel: Optional[str] = None,\n    ylabel: str = \"Normalized counts\",\n    title: Optional[str] = None,\n    output_path: Optional[str] = None,\n    stat: str = \"density\",\n) -&gt; None:\n    \"\"\"\n    Plot overlaid distributions of several variable series using histograms with optional KDE.\n\n    Parameters\n    ----------\n    series_list : list of pd.Series\n        List of pandas Series to plot.\n    labels : list of str\n        Labels for each series, used in the legend.\n    colors : list of str\n        Colors to use for each series.\n    alpha_list : list of float, optional\n        Transparency levels for each histogram. Defaults to 0.5 for all if not specified.\n    bins : int\n        Binning strategy for histograms.\n    kde : bool, optional\n        Whether to overlay a KDE curve. Default is True.\n    xlabel : str, optional\n        Label for the x-axis.\n    ylabel : str, optional\n        Label for the y-axis. Default is \"Normalized counts\".\n    title : str, optional\n        Plot title.\n    output_path : str, optional\n        Path to save the plot. If None, shows the plot interactively.\n    stat : str, optional\n        Normalization strategy. Options: \"count\", \"frequency\", \"density\", \"probability\".\n        Default is \"density\".\n    \"\"\"\n    if alpha_list is None:\n        alpha_list = [0.5] * len(series_list)\n\n    plt.figure(figsize=(10, 6))\n\n    # Compute unified bins if bins is a string like \"auto\" or a single int\n    if isinstance(bins, (int, str)):\n        combined = pd.concat(series_list)\n        min_val, max_val = combined.min(), combined.max()\n        bins_range = np.linspace(min_val, max_val, bins)\n\n    for series, label, color, alpha in zip(series_list, labels, colors, alpha_list):\n        sns.histplot(\n            series.dropna(),\n            kde=kde,\n            bins=bins_range,\n            color=color,\n            alpha=alpha,\n            stat=stat,\n            label=label,\n        )\n\n    plt.xlabel(xlabel or \"Variable\")\n    plt.ylabel(ylabel)\n    if title:\n        plt.title(title)\n    plt.legend()\n    plt.tight_layout()\n\n    if output_path:\n        plt.savefig(output_path)\n    else:\n        plt.show()\n</code></pre>"},{"location":"api/#stroke_predictorutilspreprocessing","title":"\ud83d\udcc2 <code>stroke_predictor.utils.preprocessing</code>","text":""},{"location":"api/#stroke_predictor.utils.preprocessing.drop_rows_with_missing","title":"<code>drop_rows_with_missing(df, column, missing_value)</code>","text":"<p>Drop rows with missing values in specified columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>column</code> <code>str</code> <p>Column name to check for missing values.</p> required <code>missing_value</code> <code>Any</code> <p>Value to consider as missing (e.g., np.nan).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with rows containing missing values in specified columns removed.</p> Source code in <code>stroke_predictor/utils/preprocessing.py</code> <pre><code>def drop_rows_with_missing(\n    df: pd.DataFrame, column: str, missing_value: Any\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Drop rows with missing values in specified columns.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Input DataFrame.\n    column : str\n        Column name to check for missing values.\n    missing_value : Any\n        Value to consider as missing (e.g., np.nan).\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with rows containing missing values in specified columns removed.\n    \"\"\"\n    if column not in df.columns:\n        raise ValueError(f\"Column '{column}' not found in DataFrame.\")\n\n    # Drop rows where the specified column has the missing value\n    df_dropped = df[df[column] != missing_value].copy()\n    return df_dropped\n</code></pre>"},{"location":"api/#stroke_predictor.utils.preprocessing.encode_categorical","title":"<code>encode_categorical(df, columns)</code>","text":"<p>Apply Ordinal Encoding to specified columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>columns</code> <code>list</code> <p>List of categorical columns to encode.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with encoded columns.</p> <code>OrdinalEncoder</code> <p>Fitted encoder object.</p> Source code in <code>stroke_predictor/utils/preprocessing.py</code> <pre><code>def encode_categorical(\n    df: pd.DataFrame, columns: List[str]\n) -&gt; Tuple[pd.DataFrame, OrdinalEncoder]:\n    \"\"\"\n    Apply Ordinal Encoding to specified columns.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Input DataFrame.\n    columns : list\n        List of categorical columns to encode.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with encoded columns.\n    OrdinalEncoder\n        Fitted encoder object.\n    \"\"\"\n    if not all(col in df.columns for col in columns):\n        missing_cols = [col for col in columns if col not in df.columns]\n        raise ValueError(f\"Columns {missing_cols} not found in DataFrame.\")\n\n    encoder = OrdinalEncoder()\n    df_encoded = df.copy()\n    df_encoded[columns] = encoder.fit_transform(df[columns])\n    return df_encoded, encoder\n</code></pre>"},{"location":"api/#stroke_predictor.utils.preprocessing.impute_missing_knn","title":"<code>impute_missing_knn(df, columns, n_neighbors=5)</code>","text":"<p>Impute missing values using KNN imputation.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>columns</code> <code>list</code> <p>Columns with missing values to impute.</p> required <code>n_neighbors</code> <code>int</code> <p>Number of neighbors, by default 5.</p> <code>5</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with imputed values.</p> Source code in <code>stroke_predictor/utils/preprocessing.py</code> <pre><code>def impute_missing_knn(\n    df: pd.DataFrame, columns: List[str], n_neighbors: int = 5\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Impute missing values using KNN imputation.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Input DataFrame.\n    columns : list\n        Columns with missing values to impute.\n    n_neighbors : int, optional\n        Number of neighbors, by default 5.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with imputed values.\n    \"\"\"\n    if not all(col in df.columns for col in columns):\n        missing_cols = [col for col in columns if col not in df.columns]\n        raise ValueError(f\"Columns {missing_cols} not found in DataFrame.\")\n\n    imputer = KNNImputer(n_neighbors=n_neighbors)\n    df_imputed = df.copy()\n    df_imputed[columns] = imputer.fit_transform(df[columns])\n    return df_imputed\n</code></pre>"},{"location":"api/#stroke_predictor.utils.preprocessing.replace_missing_to_value","title":"<code>replace_missing_to_value(df, column, missing_value, new_value)</code>","text":"<p>Replace missing values in specified columns with a given value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>column</code> <code>str</code> <p>Column name to process.</p> required <code>missing_value</code> <code>Any</code> <p>Value to consider as missing (e.g., np.nan).</p> required <code>new_value</code> <code>Any</code> <p>Value to replace missing values with.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with specified missing values replaced.</p> Source code in <code>stroke_predictor/utils/preprocessing.py</code> <pre><code>def replace_missing_to_value(\n    df: pd.DataFrame, column: str, missing_value: Any, new_value: Any\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Replace missing values in specified columns with a given value.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Input DataFrame.\n    column : str\n        Column name to process.\n    missing_value : Any\n        Value to consider as missing (e.g., np.nan).\n    new_value : Any\n        Value to replace missing values with.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with specified missing values replaced.\n    \"\"\"\n    if column not in df.columns:\n        raise ValueError(f\"Column '{column}' not found in DataFrame.\")\n\n    # Replace missing values in the specified column\n    df_replaced = df.copy()\n    df_replaced[column] = df_replaced[column].replace(missing_value, new_value)\n    return df_replaced\n</code></pre>"},{"location":"api/#stroke_predictor.utils.preprocessing.split_and_rebuild_dataframe","title":"<code>split_and_rebuild_dataframe(df, target_column, test_size=0.3, random_state=42, stratify=True)</code>","text":"<p>Split a DataFrame into train and test sets, and return both with the target column included.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame including the target column.</p> required <code>target_column</code> <code>str</code> <p>The name of the target column.</p> required <code>test_size</code> <code>float</code> <p>Fraction of the dataset to include in the test split. Default is 0.3.</p> <code>0.3</code> <code>random_state</code> <code>int</code> <p>Random state for reproducibility. Default is 42.</p> <code>42</code> <code>stratify</code> <code>bool</code> <p>Whether to stratify based on the target column. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame]</code> <p>Training and test sets including the target column.</p> Source code in <code>stroke_predictor/utils/preprocessing.py</code> <pre><code>def split_and_rebuild_dataframe(\n    df: pd.DataFrame,\n    target_column: str,\n    test_size: float = 0.3,\n    random_state: int = 42,\n    stratify: bool = True,\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Split a DataFrame into train and test sets, and return both with the target column included.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        The input DataFrame including the target column.\n    target_column : str\n        The name of the target column.\n    test_size : float, optional\n        Fraction of the dataset to include in the test split. Default is 0.3.\n    random_state : int, optional\n        Random state for reproducibility. Default is 42.\n    stratify : bool, optional\n        Whether to stratify based on the target column. Default is True.\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, pd.DataFrame]\n        Training and test sets including the target column.\n    \"\"\"\n    if target_column not in df.columns:\n        raise ValueError(f\"Target column '{target_column}' not found in DataFrame.\")\n\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X,\n        y,\n        test_size=test_size,\n        random_state=random_state,\n        stratify=y if stratify else None,\n    )\n\n    df_train = X_train.copy()\n    df_train[target_column] = y_train\n\n    df_test = X_test.copy()\n    df_test[target_column] = y_test\n\n    # Ensure columns are in the same order as original DataFrame\n    df_train = df_train[df.columns]\n    df_test = df_test[df.columns]\n\n    return df_train, df_test\n</code></pre>"}]}